{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de txt : 41551\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "liste_txt = glob.glob(\"tout_range/*.txt\")\n",
    "print(f\"Nombre de txt : {len(liste_txt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def ouvrir_fichier(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        chaine = f.read()\n",
    "    return chaine\n",
    "\n",
    "def decouper_mots(chaine):\n",
    "    mots = [re.sub(\"\\r|\\n|\\|\",\"\", mot) for mot in chaine.split()]\n",
    "    return mots\n",
    "\n",
    "def get_effectif(liste):\n",
    "    dic = {}\n",
    "    for mot in liste:\n",
    "        dic.setdefault(mot, 0)\n",
    "        dic[mot]+=1\n",
    "    return dic\n",
    "\n",
    "def merge_dic(dic1, dic2):\n",
    "    communs   = set(dic1.keys()).intersection(set(dic2.keys()))\n",
    "    dans_dic2 = set(dic2.keys()).difference(set(dic1.keys()))\n",
    "    for cle in communs:\n",
    "        dic1[cle]+=dic2[cle]\n",
    "    for cle in dans_dic2:\n",
    "        dic1[cle] = dic2[cle]\n",
    "    return dic1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire global : 87, Fichier courant : 87\n",
      "Vocabulaire global : 87, Fichier courant : 0\n",
      "Vocabulaire global : 122, Fichier courant : 44\n",
      "Vocabulaire global : 144, Fichier courant : 23\n",
      "Vocabulaire global : 151, Fichier courant : 10\n"
     ]
    }
   ],
   "source": [
    "dic_effectifs_global = {}\n",
    "for path in liste_txt[:5]:\n",
    "    chaine = ouvrir_fichier(path)\n",
    "    mots = decouper_mots(chaine)\n",
    "    effectifs_fichier = get_effectif(mots)\n",
    "    dic_effectifs_global = merge_dic(dic_effectifs_global, effectifs_fichier)\n",
    "    print(f\"Vocabulaire global : {len(dic_effectifs_global)}, Fichier courant : {len(effectifs_fichier)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_lexique(dic_effectifs):\n",
    "    NB_tokens = sum(dic_effectifs.values())\n",
    "    dic_out ={mot: {\"Effectif\":effectif,\"Fréquence\":effectif/NB_tokens} for mot, effectif in dic_effectifs.items()}\n",
    "    return dic_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexique_frequence = generer_lexique(dic_effectifs_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "def stocker_lexique(dic, name_base=\"lexique\"):\n",
    "    with open(f\"{name_base}.json\", \"w\", encoding=\"utf-8\") as w:\n",
    "         w.write(json.dumps(dic, ensure_ascii=False, indent=2))\n",
    "    voc_trie = sorted(list(dic.keys()))\n",
    "    with open(f\"{name_base}.txt\", \"w\", encoding=\"utf-8\") as w:\n",
    "        for mot in voc_trie:\n",
    "            w.write(\"|\".join([mot, str(dic[mot][\"Effectif\"]), str(dic[mot][\"Fréquence\"])]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocker_lexique(lexique_frequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LGERM.json\", encoding=\"utf-8\") as f:\n",
    "    LGERM = json.load(f)\n",
    "mots_LGERM = set(LGERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille Voc : 75203\n"
     ]
    }
   ],
   "source": [
    "dic_effectifs_global = {}\n",
    "for path in liste_txt[:5000]:\n",
    "    chaine = ouvrir_fichier(path)\n",
    "    mots = decouper_mots(chaine)\n",
    "    effectifs_fichier = get_effectif(mots)\n",
    "    dic_effectifs_global = merge_dic(dic_effectifs_global, effectifs_fichier)\n",
    "    \n",
    "lexique_frequence = generer_lexique(dic_effectifs_global)\n",
    "stocker_lexique(lexique_frequence, \"lexique_complet\")\n",
    "print(f\"Taille Voc : {len(lexique_frequence)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "communs   = set(lexique_frequence.keys()).intersection(mots_LGERM)\n",
    "manquants = set(lexique_frequence.keys()).difference(mots_LGERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille LGERM : 3049125\n",
      "Taille Voc : 75203\n",
      "0.10980944909112668\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taille LGERM : {len(LGERM)}\")\n",
    "print(f\"Taille Voc : {len(lexique_frequence)}\")\n",
    "taux_lex = len(communs)/len(lexique_frequence)\n",
    "print(taux_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314408\n"
     ]
    }
   ],
   "source": [
    "NB_tokens = sum(dic_effectifs_global.values())\n",
    "print(NB_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4896503905753034\n"
     ]
    }
   ],
   "source": [
    "# taux lexicalité pondéré\n",
    "eff_commun = [dic_effectifs_global[x] for x in communs]\n",
    "total = sum(eff_commun)\n",
    "taux_lex_pond = total/NB_tokens\n",
    "print(taux_lex_pond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13444, 'de'], [6499, '&'], [6400, 'la'], [6071, 'que'], [5478, 'le'], [4644, 'les'], [3129, 'qui'], [2929, 'e'], [2519, 'des'], [2414, 'ne'], [2360, 'pour'], [2321, 'en'], [2271, 'a'], [2155, 'à'], [1895, 'du'], [1865, 'ce'], [1762, 'l'], [1610, 'par'], [1589, 'vous'], [1456, 'il']]\n"
     ]
    }
   ],
   "source": [
    "l_communs = [[dic_effectifs_global[x], x] for x in communs]\n",
    "print(sorted(l_communs, reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2502, '1'], [1636, '¬'], [1439, 'qu’il'], [1367, 'ſe'], [1290, 'ſi'], [1229, '.'], [1124, 'ſon'], [1023, 'ſa'], [973, 'eſt'], [874, 'ſes'], [834, 'E'], [820, 'à'], [750, 'A'], [714, ','], [585, 'Et'], [562, 'ſur'], [547, 'ſont'], [490, 'Le'], [485, '2'], [479, '3']]\n"
     ]
    }
   ],
   "source": [
    "l_diff = [[dic_effectifs_global[x], x] for x in manquants]\n",
    "print(sorted(l_diff, reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## un lexique par type de tokenisation, on compare le taux de lexicalié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classer pages selon longueur et taux lex\n",
    "##ajouter pages témoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
